Building on the theoretical framework of Partially Observable Active Markov Games developed in the previous section, we now apply our POLARIS algorithm to concrete social learning scenarios: strategic experimentation and learning without experimentation. This implementation demonstrates how our approach bridges theoretical economic models with computational reinforcement learning methods, allowing us to validate theoretical predictions while uncovering new insights about strategic adaptation in partial observability settings.\footnote{Complete Python implementation is available at \url{https://github.com/ecdogaroglu/polaris}.}

\section{Methodological Framework and Challenges}

Theoretical frameworks —both in economics and in our POAMG framework— envision agents developing strategies that perform optimally across all possible states of the world. This \emph{ex ante} perspective requires agents to maintain flexibility and develop sophisticated mixed approaches that can respond appropriately regardless of which state is ultimately realized. Agents should learn conditional strategies of the form "if state = A, do X; if state = B, do Y," accounting for uncertainty about the true environmental state. 

The computational approach, however, reveals a fundamental departure from this theoretical ideal. Reinforcement learning agents instead converge to state-specific optimal allocations—an \emph{ex post} perspective where behavior is optimized for the particular state that has been realized. This distinction creates a critical incompatibility in the social learning environments we study, particularly due to their fixed-state structure.

To understand why this incompatibility emerges, consider the nature of optimal behavior in social learning environments with binary state spaces. In strategic experimentation, if the risky arm is good, rational agents should allocate all resources to it; conversely, if it performs poorly, optimal behavior requires complete avoidance. Similarly, in learning without experimentation, once the true state is A, the optimal action is unambiguously action A, while in state B, only action B is rational. These ex post strategies represent fundamentally incompatible policy objectives that cannot be reconciled through standard learning approaches.

The fixed-state structure of social learning models compounds this challenge. In each episode, agents can only observe and learn from one realized state, forcing them away from the ex ante strategies that theoretical frameworks prescribe. Instead of developing belief-contingent policies, agents learn "do X" during episodes with state A, then encounter conflicting objectives to "do Y" during subsequent episodes with state B. 

One natural approach to address this limitation would be to train agents across different environmental configurations, then evaluate their behavior under the full spectrum of possible conditions. However, this approach consistently fails due to a fundamental constraint of neural network learning. When network parameters are continuously updated across episodes, action-value functions adapt to the most recent state, leading to policies that are only optimal for the last state encountered. This phenomenon—known as \emph{catastrophic forgetting}—fundamentally constrains model evaluation for ex ante analysis.

\subsection{Catastrophic Forgetting in Fixed-State Social Learning}

Catastrophic forgetting represents a fundamental limitation in neural network learning where the acquisition of new knowledge systematically interferes with previously learned information \citep{vandeven2024continuallearningcatastrophicforgetting}. This phenomenon occurs when neural networks, optimized through gradient descent, overwrite previously learned weights and representations as they adapt to new tasks or environments. While various techniques have been developed to mitigate this issue in continual learning scenarios—including regularization methods, memory replay systems, and architectural approaches—these solutions typically assume some degree of compatibility or transferability between learning objectives \citep{kirkpatrick2017overcoming, zenke2017continuallearningsynapticintelligence, rebuffi2017icarl, rusu2016progressive}.


In our social learning environments, this compatibility assumption fails completely. When the optimal policy in state A requires action X while the optimal policy in state B demands the opposite action Y, no meaningful knowledge transfer is possible between episodes with different state realizations. The literature on negative transfer confirms that attempting to force knowledge sharing between fundamentally incompatible tasks degrades performance rather than enhancing it \citep{ahn2024catastrophic, wu2021online}. Traditional continual learning mechanisms that aim to "consolidate" or "protect" previous learning become counterproductive when such protection would directly undermine the acquisition of conflicting but equally necessary knowledge.

These fundamental constraints force us to reset agent parameters at the beginning of each episode rather than allowing continuous learning across state realizations—a methodological compromise that fundamentally undermines our ability to directly test the theoretical equilibrium conditions. This parameter reset prevents agents from developing the ex ante conditional strategies that both economic theory and our POAMG framework predict, instead forcing them to learn state-specific policies that cannot generalize across different environmental realizations.

Given these constraints, our experimental methodology necessarily shifts focus to within-episode learning dynamics and cross-episode pattern analysis. This indirect approach allows us to characterize certain features of multi-agent social learning, though it falls short of the direct equilibrium validation that would constitute ideal theoretical testing.

\subsection{Ex-Post Analysis Methodology}
\label{sec:ex-post-analysis}

Recognizing that catastrophic forgetting prevents direct measurement of equilibrium strategies, we implement an ex-post analysis methodology as a workaround to extract off-equilibrium learning patterns. While this approach cannot resolve the fundamental limitations discussed above, it provides a systematic framework for analyzing the learning patterns that we can observe within the constraints of our computational approach.

\paragraph{Stage 1: Within-Episode Role Identification} For each episode $e$, we identify agents that exhibit extreme performance characteristics. In strategic experimentation, we track cumulative allocation patterns:
\begin{equation}
    i_{\text{high}}^e = \arg\max_i \sum_{t=1}^T a_{t,e}^i, \quad 
    i_{\text{low}}^e = \arg\min_i \sum_{t=1}^T a_{t,e}^i
\end{equation}

In learning without experimentation, we identify the fastest and slowest learning agents by fitting learning rates:
\begin{equation}
    i_{\text{fast}}^e = \arg\max_i r_e^i, \quad 
    i_{\text{slow}}^e = \arg\min_i r_e^i
\end{equation}
where $r_e^i$ is agent $i$'s learning rate in episode $e$.

\paragraph{Stage 2: Cross-Episode Pattern Aggregation} We then aggregate the performance of identified extreme agents across episodes to examine systematic patterns:
\begin{equation}
    \bar{P}_t^{\text{extreme}} = \frac{1}{n_{\text{episodes}}} \sum_{e=1}^{n_{\text{episodes}}} P_{t,e}^{i_{\text{extreme}}^e}
\end{equation}
where $P_{t,e}^i$ represents the relevant performance metric (cumulative allocation, learning accuracy, etc.) for agent $i$ at time $t$ in episode $e$.

This methodology provides a limited window into multi-agent learning patterns, allowing us to examine whether observed disparities represent structural features that persist across episodes rather than artifacts of specific agent characteristics.

\section{Strategic Experimentation}

We reformulate the strategic experimentation model of \citet{keller2020undiscounted} as a Partially Observable Active Markov Game. This framework analyzes undiscounted continuous-time games where a number of symmetric players act non-cooperatively, trying to learn an unknown state of the world governing the risky arm's expected payoff. The state space $S = \{0,1,\ldots,m\}$ represents possible states of the world, with a deterministic transition function since the underlying state remains constant throughout the interaction. Each agent's action space in the theoretical model is $A^i = [0,1]$ representing the fraction of resources allocated to the risky arm at each decision point.

While the original model operates in continuous time with Lévy processes, our POAMG implementation discretizes time using the Euler-Maruyama scheme \citep{kloeden1992numerical, platen1999introduction} for both the background signal and the agents' payoff processes. However, to enable reinforcement learning with discrete action spaces, we implement a practical discretization where agents choose between two actions: "allocate to risky arm" (action 1) or "allocate to safe arm" (action 0). The probability of selecting action 1 serves as the continuous allocation parameter, allowing us to recover the continuous allocation behavior while maintaining compatibility with discrete policy networks.

The agents receive a public signal produced by the background process, which leads to the observation function:

\begin{equation}
    O^i(s, o) = \mathbb{P}(o_t | B_{t-1:t})
\end{equation}

where $o_t$ represents the observation at discrete time step $t$ and $B_{t-1:t}$ represents the signal increment in discrete time. To address the time-dependent nature of Lévy processes while maintaining compatibility with our POAMG framework, we formulate the reward function for each agent $i$ as:

\begin{equation}
    R^i(s, a^i) = (1-a^i)r_\textit{safe} + a^i \frac{X^i_{t-1:t}}{\Delta t}
\end{equation}
where $\Delta t$ is the discretization time step,
\begin{equation}
    X^i_{t-1:t} = \alpha_s \Delta t + \sigma (W^i_t - W^i_{t-1}) + \Delta Y^i_t,
\end{equation}
$(W^i_t - W^i_{t-1}) \sim \mathcal{N}(0, \Delta t)$, and $\Delta Y^i_t$ is the increment of the compound Poisson processes over the interval $[t-1, t]$.
This normalization converts accumulated rewards to instantaneous reward rates, preserving the incentive structure of the original model. A comprehensive mathematical treatment of this discretization approach, including convergence properties and the preservation of strategic incentives, is provided in Appendix \ref{appendix:levy_discretization}.

Each agent observes the increments in the public background signal, their own payoff process (dependent on their allocation $a^i$ to the risky arm and the true state $\omega$), and the allocations together with the rewards of all other agents. For continuous signals, POLARIS agents use a specialized transformer loss function that computes the expected signal distribution based on the current belief state and environment parameters, enabling principled belief updating in response to Lévy process observations.

\subsection{Experimental Setup}

We implement comprehensive experiments using specialized experimental frameworks for both detailed single-configuration analysis and comparative analysis across different group sizes. Our experimental protocol examines group sizes of $n \in \{1, 2, 4, 8\}$ agents; safe payoff of $r_{\text{safe}} = 0.5$; drift rates of $[0, 1]$ for bad and good states respectively; jump rates of $[0, 0.1]$ with jump sizes of $[1.0, 1.0]$; background informativeness of $k_0 = 0.001$; and time discretization step of $\Delta t = 1.0$.

Following our ex-post analysis methodology, the POLARIS implementation tracks: (1) Convergence of individual agent allocation strategies to state-dependent optimal actions within episodes; (2) Belief fluctuations over time to assess the impact of neighbor action observation on learning; (3) Cumulative allocation disparities between highest and lowest allocators as a function of group size; and (4) Episode-wise analysis of allocation patterns across different true states (good vs. bad).

For each configuration, we run experiments for comparative sweeps across group sizes, focusing on the emergence of allocation patterns and their relationship to theoretical predictions. Our analysis emphasizes within-episode learning dynamics rather than cross-episode equilibrium convergence due to the catastrophic forgetting constraint.

\subsection{Results and Analysis}

Our experimental results reveal sophisticated strategic learning dynamics while highlighting key challenges in measuring theoretical equilibrium convergence in reinforcement learning settings. We examine three primary aspects of the learning behavior that provide insights into multi-agent strategic experimentation.

\begin{figure}[!htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../code/polaris/docs/allocation_accuracy_over_time.png}
        \subcaption{Allocation accuracy over time.}
        \label{fig:allocation_accuracy_over_time}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../code/polaris/docs/belief_accuracy_over_time.png}
        \subcaption{Belief accuracy over time.}
        \label{fig:belief_accuracy_over_time}
    \end{minipage}
    \caption{Learning dynamics in the strategic experimentation model}
    \label{fig:strategic_experimentation_results}
\end{figure}

\paragraph{Strategic Action Convergence} POLARIS agents successfully learn to converge toward state-dependent optimal allocation strategies. In good states, agents increase their allocation to the risky arm over time, while in bad states, they learn to favor the safe alternative (Figure \ref{fig:allocation_accuracy_over_time}). This convergence demonstrates that the discretized action representation effectively captures the continuous allocation dynamics of the theoretical model, enabling agents to discover appropriate exploration-exploitation strategies through reinforcement learning.

\paragraph{Belief Dynamics and Social Learning Effects} Figure \ref{fig:belief_accuracy_over_time} reveals fluctuating belief patterns alongside stable allocation policies. This demonstrates how POLARIS agents incorporate social information through the inference module while maintaining strategic coherence. The belief variability reflects ongoing social learning as agents observe noisy private signals, yet their policies converge reliably toward optimal state-specific strategies.

\begin{figure}[!htbp]
    \centering
        \includegraphics[width=\textwidth]{../code/polaris/docs/highest_lowest_cumulative_allocators_good_state.png}
        \caption{Highest and lowest cumulative allocators in the good state. (Average over 20 episodes)}
        \label{fig:highest_lowest_cumulative_allocators_good_state}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{../code/polaris/docs/highest_lowest_cumulative_allocators_bad_state.png}
    \caption{Highest and lowest cumulative allocators in the bad state. (Average over 20 episodes)}
    \label{fig:highest_lowest_cumulative_allocators_bad_state}
\end{figure}

\paragraph{Dynamic Role Assignment and Allocation Disparities} Figures \ref{fig:highest_lowest_cumulative_allocators_good_state} and \ref{fig:highest_lowest_cumulative_allocators_bad_state} demonstrate a systematic widening of the gap between highest and lowest cumulative allocators as group size increases. This pattern reveals the emergence of dynamic role assignment in multi-agent learning, where agents naturally differentiate into information generators and information exploiters within each episode. In strategic experimentation, this manifests as some agents reducing individual experimentation when benefiting from others' information generation (free-riding effects), while enabling top experimenters to allocate more aggressively when supported by group learning (encouragement effects). Interestingly, despite the presence of free-riding behavior typically associated with inefficiency in economic literature, our results show that cumulative allocation in larger networks substantially exceeds autarky levels in both good and bad states, suggesting that the social learning benefits and encouragement effects more than compensate for the traditional inefficiencies of free-riding in these multi-agent learning environments.

\paragraph{Implications and Validation} These results demonstrate that our POAMG framework captures essential features of strategic experimentation dynamics, including state-dependent learning, social information effects, and group size impacts on allocation behavior. The observed patterns of strategic adaptation, belief fluctuations, and allocation disparities provide computational validation for key theoretical insights about multi-agent experimentation and information sharing, even though direct equilibrium convergence measurement is precluded by the catastrophic forgetting constraint discussed in our methodological framework.

\section{Learning Without Experimentation}

We reformulate the learning without experimentation model of \citet{brandl2024} as a Partially Observable Active Markov Game. In this formulation, the state space $S = \{s^1, s^2, \ldots, s^m\}$ represents possible states of the world, with a deterministic transition function since the state remains constant. Each agent's action space $A^i = \{a^1, a^2, \ldots, a^m\}$ corresponds to potential states, where $a^j$ is the unique optimal action is state $s^j$. In addition to observing their neighbors' actions, each agent receives a private signal $o^i_t \in \Omega^i = S$ drawn from distribution:

\begin{equation*}
    O^i(s,o) =
    \begin{cases}
        q               & \text{if } s = o \\
        \frac{1-q}{m-1} & \text{otherwise}
    \end{cases}
\end{equation*}
where $q > 1/m$ is the signal accuracy. Since agents don't observe real rewards in this model, we construct an observed reward function that facilitates learning:

\begin{equation*}
    R^i(s_t, a^i_t) = v(o^i_t, a^i_t) = \frac{q \cdot \mathbb{I}_{\{a^i_t = \varphi(o^i_t)\}} - (1 - q) \cdot \mathbb{I}_{\{a^i_t \neq \varphi(o^i_t)\}}}{2q - 1}
\end{equation*}

where $\varphi$ maps states to their corresponding correct actions.\footnote{Formally, the mapping $\varphi: S \rightarrow A$ is a bijective function defined as $\varphi(s^j) = a^j$ for all $j \in \{1,2,...,m\}$, associating each state with the action having the same index.} This construction ensures the expected reward matches the true utility function in expectation: \footnote{See \ref{appendix:observed_reward_derivation} for a detailed derivation and the formulation for a generic dimension $m$.}

\begin{equation*}
    \mathbb{E}_{o \sim \mu^{s}}[v(o, a)] = u(s, a) = \mathbb{I}_{\{a = \varphi(s)\}}.
\end{equation*}


In this setup, agents are rewarded based on their posterior belief given the private signal, maintaining compatibility with the original model's incentive structure while providing a learning signal for POLARIS.

\subsection{Experimental Setup}

We implement comprehensive experiments to validate these theoretical predictions and explore the learning dynamics under various network structures. Our experimental protocol examines network sizes of $n \in \{1, 2, 4, 8\}$ agents; network topologies including complete, ring, star, and random networks; signal accuracy of $p = 0.75$ \iffalse(we use the same signal accuracy as in the paper's example where $r_{\text{aut}} \approx 0.14$, $r_{\text{crd}} \approx 0.55$, and $r_{\text{bdd}} \approx 1.10$)\fi; and learning metrics consisting of empirical learning rates extracted via log-linear regression.

Following our ex-post analysis methodology described in Section \ref{sec:ex-post-analysis}, we track each agent's incorrect action probability as the probability assignment of the policy network to the incorrect action over time and estimate individual learning rates by fitting log-linear regressions to identify the fastest and slowest learners within each episode. We then aggregate the performance of these extreme learners across episodes to examine whether the observed learning disparities represent structural features of multi-agent learning rather than artifacts of specific agent characteristics.

\subsection{Results and Analysis}
\iffalse
Our experimental results provide strong validation of the theoretical predictions while revealing additional insights into the learning dynamics. We begin by addressing a key methodological consideration before examining our main findings.

\paragraph{Methodological Context} The theoretical autarky rate of $r_{\text{aut}} \approx 0.14$ (derived using large deviation theory for random walks) differs substantially from the empirical single-agent POLARIS performance of approximately $0.35$ (Figure \ref{fig:size_comparison}). This difference reflects fundamental methodological distinctions: the theoretical analysis assumes agents follow the maximum likelihood strategy with perfect signal processing, while POLARIS agents learn policies through reinforcement learning with our constructed reward function and Transformer-based processing. Additionally, the theoretical rate derives from asymptotic analysis as $t \to \infty$, whereas our empirical measurements occur over finite horizons. Despite these methodological differences, both approaches provide complementary insights into learning performance bounds and achievable rates in practice.
\fi

Our analysis reveals three key phenomena that validate and extend the theoretical predictions: the emergence of learning barriers, the realization of coordination benefits, and the dynamic formation of information revelation roles.


\paragraph{Learning Barriers} Across all network configurations and episodes, we consistently observe a significant reduction in the slowest-learning agent's rate compared to the autarky rate. Figure \ref{fig:size_comparison} demonstrates this barrier effect by aggregating the trajectories of whichever agent learns slowest in each episode. This finding directly supports the theoretical prediction of the paper. While we cannot observe whether the bound significantly exceeds the autarky rate as the theorem suggests—such validation would require much larger networks and higher computational resources—the consistent emergence of learning barriers across all configurations provides strong empirical support for the theoretical framework.

\paragraph{Coordination Benefits} In networks with $n \geq 4$ agents, the fastest-learning agents consistently demonstrate superior performance compared to isolated agents, confirming the coordination benefits predicted by the paper. This demonstrates that social learning enables some agents to surpass single-agent performance, though at the expense of others. The systematic nature of this improvement across different network sizes suggests that the coordination benefits are robust features of multi-agent social learning rather than artifacts of specific configurations. However, similar to our findings on learning barriers, we cannot observe whether the learning rate of the slowest-learning agent can surpass the autarky rate for sufficiently large networks as the theorem suggests.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{../code/polaris/docs/fastest_slowest_network_sizes_evolution.png}
    \caption{Fastest vs slowest agent learning trajectories across network sizes.}
    \label{fig:size_comparison}
\end{figure}

\paragraph{Information Revelation Through Dynamic Role Assignment} Our results reveal a systematic pattern where agents naturally emerge as either information generators or information exploiters within each episode—a phenomenon that parallels the dynamic role assignment observed in strategic experimentation, where agents differentiate into high and low allocators. In learning without experimentation, the agents that learn slowest exhibit higher variance in their action choices (evident from the wider confidence intervals in Figure \ref{fig:size_comparison}), indicating more exploratory behavior that enables other agents to make better-informed decisions. This mirrors the information generation role played by high allocators in strategic experimentation, who provide valuable learning signals through their experimental choices.

\iffalse
Crucially, this division of labor emerges dynamically in both scenarios—different agents assume these roles across episodes (Figure \ref{fig:slowest_agent_network_positions}), suggesting the phenomenon arises from learning dynamics rather than fixed agent characteristics. This consistency across fundamentally different social learning environments demonstrates that dynamic role assignment represents a robust feature of multi-agent learning systems, where complementary specialization enhances collective information processing regardless of whether the mechanism operates through allocation patterns or exploration variance.
\fi


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{../code/polaris/docs/fastest_slowest_network_types_evolution.png}
    \caption{Fastest vs slowest agent learning trajectories across network topologies.}
    \label{fig:type_comparison}
\end{figure}

\paragraph{Network Structure \& Information Flow} Figure \ref{fig:type_comparison} reveals how network topology influences learning dynamics. Complete networks facilitate the most effective information sharing, producing the largest performance gaps between fastest and slowest learners. Ring networks show more uniform learning rates due to limited information flow, while star networks create pronounced differences between central and peripheral agents. These patterns align with theoretical insights that information aggregation depends critically on network structure, though the specific mechanisms through which topology affects learning warrant further investigation.
\iffalse
To understand the underlying mechanisms determining which agents become information generators, we examine the roles of network position and signal quality—two factors that could potentially explain the emergence of learning disparities.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{../code/polaris/docs/slowest_agent_network_positions.png}
    \caption{The position of the slowest-learning agent in the network over time. The frequency of being the slowest-learning agent is calculated as the average number of episodes that this position is occupied by the slowest-learning agent. The slowest-learning agent is the agent that learns the slowest in each episode.}
    \label{fig:slowest_agent_network_positions}
\end{figure}

\subparagraph{Determinants of Learning Roles} Our analysis reveals that network position alone cannot predict learning performance. Figure \ref{fig:slowest_agent_network_positions} shows that the identity of the slowest-learning agent distributes approximately uniformly across network positions, indicating that structural network advantages do not systematically determine learning performance. This finding suggests that network topology alone cannot predict which agents will act as information generators.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{../code/polaris/docs/signal_quality_vs_learning_performance.png}
    \caption{The learning performance of POLARIS agents as a function of the signal quality. Each point corresponds to a single agent in a single episode. The signal quality is calculated as the average correctness of the private signal within the episode.}
    \label{fig:signal_quality_vs_learning_performance}
\end{figure}

In contrast, Figure \ref{fig:signal_quality_vs_learning_performance} demonstrates a strong positive correlation between signal quality and learning performance. Agents who receive lower-quality private signals consistently exhibit slower learning rates, indicating that the primary determinant of becoming a slow learner is the stochastic realization of signal quality rather than strategic positioning within the network. Agents receiving more accurate private signals naturally learn faster, while those receiving noisier signals become information generators whose exploratory behavior, though individually costly, benefits the overall network through enhanced information revelation.\fi

\paragraph{Implications and Validation} These results demonstrate that our POAMG framework successfully captures the essential features of social learning without experimentation, validating both the learning barriers that limit information aggregation and the coordination benefits that emerge in well-connected networks. The computational approach complements theoretical analysis by revealing the dynamics of strategy discovery and the distributional properties of learning rates across different network configurations.

Our computational analysis reveals a striking consistency in the emergence of dynamic role assignment across fundamentally different social learning environments. In both strategic experimentation and learning without experimentation, agents naturally differentiate into complementary roles that enhance collective information processing, though the specific mechanisms differ substantially. In strategic experimentation, this differentiation manifests through allocation patterns, where some agents assume higher-risk experimental roles by allocating more resources to uncertain alternatives, while others free-ride on this information generation. In learning without experimentation, the same underlying phenomenon emerges through learning rate disparities and exploration variance, where slower-learning agents exhibit more exploratory behavior that benefits faster-learning agents who can exploit this information revelation. Crucially, these roles are not fixed by agent characteristics or network positions but emerge dynamically through the learning process itself, with different agents assuming information generation and exploitation roles across episodes. Remarkably, in both environments, the presence of free-riding behavior does not lead to the inefficiencies typically predicted by economic theory—cumulative allocations in strategic experimentation and learning rates in learning without experimentation both exceed autarky levels in larger networks, demonstrating that for some agents, social learning benefits and encouragement effects consistently outweigh traditional free-riding costs. This robustness across distinct social learning mechanisms suggests that dynamic role assignment represents a fundamental organizing principle of multi-agent learning systems, enabling some agents to efficiently aggregate information through spontaneous specialization in exploratory behavior.



