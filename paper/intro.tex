The study of how individuals and groups learn from each other's actions and experiences has been a central focus in economic theory for decades. From the seminal contributions on information cascades by \citet{banerjee1992simple} and \citet{bikhchandani1992theory} to more recent explorations of learning in networks \citep{acemoglu2011bayesian, golub2010naive}, economic research has sought to understand how social interactions shape beliefs, decisions, and collective outcomes. Traditional economic models have provided valuable insights into phenomena such as herding behavior, where individuals rationally ignore their private information to follow the observed actions of predecessors. However, these models often rely on simplified assumptions about agents' behavior and reasoning processes, frequently restricting analysis to one-shot sequential decisions or steady-state equilibria rather than capturing the rich dynamics of repeated interactions where agents continuously adapt their strategies based on others' evolving behaviors.

Meanwhile, the field of multi-agent reinforcement learning (MARL) has experienced remarkable progress in developing algorithms that enable autonomous agents to learn effective strategies in complex, interactive environments. Recent advances in reinforcement learning have demonstrated impressive capabilities in strategic games \citep{silver2016mastering}, cooperative problem-solving \citep{baker2020emergentreciprocityteamformation}, and competitive scenarios \citep{openai2019dota2largescale}.  MARL offers powerful tools for modeling adaptive behavior in non-stationary environments where agents must continuously revise their strategies in response to others' changing behaviors. However, much of this work has focused on engineering objectives rather than modeling realistic human learning processes, limiting its applicability to fundamental questions in economic theory. Additionally, many MARL approaches struggle with the challenge of partial observability—a defining feature of social learning contexts where agents cannot directly observe others' private information or belief states.

This thesis bridges these domains by introducing a novel framework that integrates economic social learning theory with multi-agent reinforcement learning under partial observability. Our approach models social learning as a dynamic process where agents with limited information strategically adapt their behaviors while simultaneously learning about their environment and other agents' strategies. Through the development of Partially Observable Active Markov Games (POAMGs) and the POLARIS algorithm, we demonstrate how computational approaches can complement traditional economic theory while revealing new insights into complex social learning dynamics, particularly the emergence of dynamic role assignment as a fundamental organizing principle of multi-agent learning systems.

The first challenge concerns non-stationarity and strategic adaptation. Traditional economic models of social learning typically assume that agents follow fixed, myopic decision rules or Bayesian updating procedures that do not fully account for strategic adaptation over time. When agents repeatedly interact and observe each other's actions, however, they may adjust their strategies in anticipation of others' learning, creating a complex web of interdependent adaptation that fundamentally alters collective learning dynamics. This non-stationarity—where the effective environment an agent faces changes as other agents learn—represents a central challenge that requires new modeling approaches. Standard MARL algorithms often treat this non-stationarity as a technical obstacle to convergence rather than an intrinsic feature of multi-agent systems that agents should explicitly reason about and strategically exploit.

The second challenge involves partial observability, which pervades social learning contexts. In most real-world settings, agents cannot directly observe others' private information, beliefs, or decision processes. Instead, they must infer these hidden states from observable actions and outcomes. This partial observability compounds the complexity of strategic interaction, as agents must simultaneously learn about the underlying state of the world and about the belief formation processes of others. Economic models have often simplified this challenge through strong assumptions about common knowledge or by focusing on one-shot sequential decisions rather than repeated interactions. Similarly, many MARL approaches assume full observability of the state or treat partial observability as a technical challenge to be addressed through belief state tracking, without fully incorporating its strategic implications.

The third challenge concerns long-term strategic considerations in social learning. Agents often face a tension between immediate payoffs and long-term information generation. They may sometimes choose actions that appear suboptimal in the short term to influence the learning trajectories of others or to generate valuable information for future periods. For instance, in strategic experimentation contexts, agents might incur costs to explore unknown options, knowing that the resulting information will benefit both themselves and others in the future. Capturing these farsighted strategic considerations requires models that explicitly account for how current actions shape the future evolution of others' beliefs and behaviors—a capability that neither traditional economic models nor standard MARL algorithms fully provide.

To address these challenges, this thesis makes four interconnected contributions that advance both economic theory and multi-agent reinforcement learning. First, we develop Partially Observable Active Markov Games (POAMGs)—a novel formalism that extends the Active Markov Game framework introduced by \citet{kim2022influencing} to partially observable settings. Unlike standard reinforcement learning frameworks that treat non-stationarity as a challenge to be mitigated, POAMGs incorporate policy evolution as an integral part of the environment dynamics, allowing agents to reason about and strategically influence this evolution process. POAMGs explicitly model how agents' policies evolve over time based on observations and interactions, while accounting for the fundamental constraints imposed by partial observability.

Second, we provide a thorough theoretical analysis of convergence and equilibrium properties in POAMGs. We establish conditions under which the joint process of states, beliefs, and policy parameters converges to a unique stochastically stable distribution, ensuring that our models have well-defined limiting behavior despite the inherent non-stationarity of multi-agent learning. We derive policy gradient theorems for average and discounted reward objectives and extend our framework to continuous-time dynamics through stochastic differential equations. Critically, we reveal how game-theoretic equilibrium concepts relate to active equilibrium when agents account for others' learning, illuminating the fundamental tension between theoretical ex ante strategies and the ex post strategies that emerge through reinforcement learning.

Third, we introduce POLARIS (Partially Observable Learning with Active Reinforcement In Social environments), combining belief processing through Transformer models, variational inference learning with Graph Neural Networks (GNNs), and reinforcement learning optimization. Our implementation incorporates GNNs with temporal and spatial attention mechanisms that capture network topology and temporal dependencies, enabling agents to develop sophisticated strategies accounting for both environmental partial observability and strategic adaptation. POLARIS addresses the critical challenge of catastrophic forgetting through an ex-post analysis methodology that provides systematic approaches for extracting insights from multi-agent learning dynamics despite agents' inability to maintain stable policies across all states.

Fourth, we validate our framework through strategic experimentation and learning without experimentation applications, building on the models of \citet{bolton1999strategic}, \citet{keller2020undiscounted}, \citet{huang2024learning}, and \citet{brandl2024}. Our analysis demonstrates dynamic role assignment as a robust organizing principle, where agents naturally differentiate into complementary roles enhancing collective information processing. Contrary to theoretical predictions about free-riding inefficiencies, we show that performance of some agents in larger networks substantially exceeds autarky levels, revealing opportunities for more realistic behavioral models that account for computational agents' learning dynamics.

A key methodological innovation is our discretization approach for continuous-time economic models, which treats Lévy processes through Euler-Maruyama schemes and maps continuous decisions to discrete action probabilities. This provides computational implementation templates while preserving essential incentive structures from theoretical models. We construct observed reward functions that enable reinforcement learning without direct reward signals and develop specialized transformer loss functions for Lévy process observations. Our theoretically grounded off-equilibrium asymmetric analysis methods move beyond symmetric equilibria analysis, allowing heterogeneous agent behaviors and asymmetric strategy convergence—capabilities essential for understanding real-world social learning dynamics.

The insights gained advance theoretical knowledge and inform the design of platforms and institutions that facilitate efficient collective learning. Our finding that dynamic role assignment emerges as a natural organizing principle suggests that effective social learning systems develop specialized roles enhancing collective information processing, providing guidance for platform design and institutional arrangements in financial markets, organizational learning, online social networks, and collaborative scientific endeavors. By demonstrating the value of MARL techniques for addressing classical questions in economic theory while revealing computational challenges like catastrophic forgetting, we contribute to both fields and encourage further cross-disciplinary work leveraging complementary insights from these rich intellectual traditions.

The remainder of this thesis is organized as follows: Chapter 2 reviews the relevant literature on social learning in economic theory and multi-agent reinforcement learning, emphasizing the complementary strengths and limitations of these approaches and identifying opportunities for cross-fertilization. Chapter 3 introduces our theoretical framework, developing the formalism of Partially Observable Active Markov Games and deriving key results on convergence, policy gradients, and equilibrium concepts, as well as presenting the POLARIS algorithm, detailing its components, implementation considerations, and theoretical justifications. Chapter 4 applies our framework to strategic experimentation and learning without experimentation, demonstrating how POLARIS captures sophisticated social learning dynamics in these contexts. Chapter 5 concludes with a discussion of implications, limitations, and directions for future research.

By integrating perspectives from economic theory and multi-agent reinforcement learning, this thesis aims to enhance our understanding of social learning processes and provide new tools for modeling complex strategic interactions in partially observable environments. The insights gained may not only advance theoretical knowledge but also inform the design of platforms and institutions that facilitate efficient collective learning and decision-making in diverse contexts, from financial markets and organizational learning to online social networks and collaborative scientific endeavors. Moreover, by demonstrating the value of MARL techniques for addressing classical questions in economic theory, we hope to encourage further cross-disciplinary work that leverages complementary insights from these rich intellectual traditions.

In the next chapter, we provide a detailed review of the relevant literature, tracing the development of social learning models in economics and recent advances in multi-agent reinforcement learning. This review establishes the conceptual foundation for our theoretical framework and highlights the specific gaps that our approach aims to address, setting the stage for the formal development of Partially Observable Active Markov Games in Chapter 3.