The study of how individuals and groups learn from each other's actions and experiences has been a central focus in economic theory for decades. From the seminal contributions on information cascades by \citet{banerjee1992simple} and \citet{bikhchandani1992theory} to more recent explorations of learning in networks \citep{acemoglu2011bayesian, golub2010naive}, economic research has sought to understand how social interactions shape beliefs, decisions, and collective outcomes. Traditional economic models have provided valuable insights into phenomena such as herding behavior, where individuals rationally ignore their private information to follow the observed actions of predecessors. However, these models often rely on simplified assumptions about agents' behavior and reasoning processes, frequently restricting analysis to one-shot sequential decisions or steady-state equilibria rather than capturing the rich dynamics of repeated interactions where agents continuously adapt their strategies based on others' evolving behaviors.

Meanwhile, the field of multi-agent reinforcement learning (MARL) has experienced remarkable progress in developing algorithms that enable autonomous agents to learn effective strategies in complex, interactive environments. Recent advances in reinforcement learning have demonstrated impressive capabilities in strategic games \citep{silver2016mastering}, cooperative problem-solving \citep{baker2020emergentreciprocityteamformation}, and competitive scenarios \citep{openai2019dota2largescale}.  MARL offers powerful tools for modeling adaptive behavior in non-stationary environments where agents must continuously revise their strategies in response to others' changing behaviors. However, much of this work has focused on engineering objectives rather than modeling realistic human learning processes, limiting its applicability to fundamental questions in economic theory. Additionally, many MARL approaches struggle with the challenge of partial observability—a defining feature of social learning contexts where agents cannot directly observe others' private information or belief states.

This thesis bridges these domains by introducing a novel framework that integrates economic social learning theory with multi-agent reinforcement learning under partial observability. Our approach models social learning as a dynamic process where agents with limited information strategically adapt their behaviors while simultaneously learning about their environment and other agents' strategies. By formalizing this process through the lens of partially observable reinforcement learning, we aim to reveal new insights into how strategic adaptation influences collective outcomes and equilibrium behavior. This interdisciplinary synthesis addresses fundamental challenges that have constrained previous work in both economics and artificial intelligence, offering a more realistic and computationally tractable approach to modeling complex social learning dynamics.

The first challenge concerns non-stationarity and strategic adaptation. Traditional economic models of social learning typically assume that agents follow fixed, myopic decision rules or Bayesian updating procedures that do not fully account for strategic adaptation over time. When agents repeatedly interact and observe each other's actions, however, they may adjust their strategies in anticipation of others' learning, creating a complex web of interdependent adaptation that fundamentally alters collective learning dynamics. This non-stationarity—where the effective environment an agent faces changes as other agents learn—represents a central challenge that requires new modeling approaches. Standard MARL algorithms often treat this non-stationarity as a technical obstacle to convergence rather than an intrinsic feature of multi-agent systems that agents should explicitly reason about and strategically exploit.

The second challenge involves partial observability, which pervades social learning contexts. In most real-world settings, agents cannot directly observe others' private information, beliefs, or decision processes. Instead, they must infer these hidden states from observable actions and outcomes. This partial observability compounds the complexity of strategic interaction, as agents must simultaneously learn about the underlying state of the world and about the belief formation processes of others. Economic models have often simplified this challenge through strong assumptions about common knowledge or by focusing on one-shot sequential decisions rather than repeated interactions. Similarly, many MARL approaches assume full observability of the state or treat partial observability as a technical challenge to be addressed through belief state tracking, without fully incorporating its strategic implications.

The third challenge concerns long-term strategic considerations in social learning. Agents often face a tension between immediate payoffs and long-term information generation. They may sometimes choose actions that appear suboptimal in the short term to influence the learning trajectories of others or to generate valuable information for future periods. For instance, in strategic experimentation contexts, agents might incur costs to explore unknown options, knowing that the resulting information will benefit both themselves and others in the future. Capturing these farsighted strategic considerations requires models that explicitly account for how current actions shape the future evolution of others' beliefs and behaviors—a capability that neither traditional economic models nor standard MARL algorithms fully provide.

To address these challenges, this thesis makes several interconnected contributions. First, we develop Partially Observable Active Markov Games (POAMGs)—a novel formalism that extends the Active Markov Game framework introduced by \citet{kim2022influencing} to partially observable settings. Unlike standard reinforcement learning frameworks that treat non-stationarity as a challenge to be mitigated, POAMGs incorporate policy evolution as an integral part of the environment dynamics, allowing agents to reason about and strategically influence this evolution process. POAMGs explicitly model how agents' policies evolve over time based on observations and interactions, while accounting for the fundamental constraints imposed by partial observability. This formalism provides a mathematically rigorous foundation for analyzing complex social learning dynamics that involve repeated interactions, adaptive strategies, and incomplete information.

Second, we provide a thorough theoretical analysis of convergence and equilibrium properties in POAMGs. We establish conditions under which the joint process of states, beliefs, and policy parameters converges to a unique stochastically stable distribution, ensuring that our models have well-defined limiting behavior despite the inherent non-stationarity of multi-agent learning. We derive policy gradient theorems for average and discounted reward objectives, providing a solid foundation for algorithm development. We also extend our framework to continuous-time dynamics through stochastic differential equations, enabling a wider range of applications. Our analysis illuminates the relationship between traditional game-theoretic equilibrium concepts and the more general notion of active equilibrium that emerges in our framework, offering new insights into equilibrium selection and stability in social learning contexts.

Third, we introduce POLARIS (Partially Observable Learning with Active Reinforcement In Social environments), a practical algorithm for learning in POAMGs. POLARIS extends previous algorithms to partially observable settings through an integrated architecture with three core components: a belief processing module that tracks agents' information states using Transformer models; an inference learning module that predicts the policy evolution of other agents through variational methods; and a reinforcement learning module that optimizes policies based on average or discounted reward criteria, supporting both discrete and continuous action spaces. This integrated approach enables agents to learn sophisticated strategies that account for both the partial observability of the environment and the learning dynamics of other agents—capabilities that are essential for modeling realistic social learning processes.

Fourth, we apply our framework to two canonical social learning scenarios from economic theory. The first application focuses on strategic experimentation, building on the models of \citet{bolton1999strategic} and \citet{keller2020undiscounted}, where agents learn optimal actions through trial and error with observable rewards. The second application examines learning without experimentation, extending the frameworks of \citet{huang2024learning} and \citet{brandl2024}, where agents form beliefs primarily by observing others' actions rather than receiving direct feedback from the environment. Our analysis demonstrates how our approach can model phenomena such as free-riding on others' information production, strategic teaching, information cascades, and confounded learning under more realistic assumptions about agents' reasoning capabilities and strategic sophistication.

Through these applications, we illustrate how bridging economic theory and MARL can enhance our understanding of social learning processes. Our POAMG framework provides a more realistic model of how partially rational agents learn from each other in complex, partially observable environments. At the same time, it offers practical algorithms for computing equilibrium strategies that traditional economic approaches might find intractable. The POLARIS algorithm demonstrates how techniques from deep reinforcement learning and approximate inference can be adapted to tackle the specific challenges of social learning, offering a computationally feasible method for simulating and analyzing complex multi-agent learning dynamics.

The remainder of this thesis is organized as follows: Chapter 2 reviews the relevant literature on social learning in economic theory and multi-agent reinforcement learning, emphasizing the complementary strengths and limitations of these approaches and identifying opportunities for cross-fertilization. Chapter 3 introduces our theoretical framework, developing the formalism of Partially Observable Active Markov Games and deriving key results on convergence, policy gradients, and equilibrium concepts, as well as presenting the POLARIS algorithm, detailing its components, implementation considerations, and theoretical justifications. Chapter 4 applies our framework to strategic experimentation and learning without experimentation, demonstrating how POLARIS captures sophisticated social learning dynamics in these contexts. Chapter 5 concludes with a discussion of implications, limitations, and directions for future research.

By integrating perspectives from economic theory and multi-agent reinforcement learning, this thesis aims to enhance our understanding of social learning processes and provide new tools for modeling complex strategic interactions in partially observable environments. The insights gained may not only advance theoretical knowledge but also inform the design of platforms and institutions that facilitate efficient collective learning and decision-making in diverse contexts, from financial markets and organizational learning to online social networks and collaborative scientific endeavors. Moreover, by demonstrating the value of MARL techniques for addressing classical questions in economic theory, we hope to encourage further cross-disciplinary work that leverages complementary insights from these rich intellectual traditions.

In the next chapter, we provide a detailed review of the relevant literature, tracing the development of social learning models in economics and recent advances in multi-agent reinforcement learning. This review establishes the conceptual foundation for our theoretical framework and highlights the specific gaps that our approach aims to address, setting the stage for the formal development of Partially Observable Active Markov Games in Chapter 3.