This thesis bridges economic social learning theory and multi-agent reinforcement learning by developing Partially Observable Active Markov Games (POAMGs) and the POLARIS algorithm. Our framework addresses fundamental challenges in modeling adaptive behavior under partial observability, demonstrating how computational approaches complement traditional economic theory while providing new insights into complex social learning dynamics.

\paragraph{Summary of Contributions}

Our work makes four interconnected contributions. First, we developed the POAMG framework, extending Active Markov Games to partially observable settings where agents reason about environmental uncertainty and evolving strategies of others. Second, we provided theoretical analysis of convergence and equilibrium properties, establishing stochastic stability conditions and deriving policy gradient theorems for average and discounted rewards. We extended the framework to continuous-time dynamics through stochastic differential equations and revealed how game-theoretic equilibrium concepts relate to active equilibrium when agents account for others' learning. Third, we introduced POLARIS, combining belief processing through Transformer models, variational inference learning with GNNs, and reinforcement learning optimization, enabling agents to develop sophisticated strategies accounting for both environmental partial observability and strategic adaptation. Fourth, we validated our framework through strategic experimentation and learning without experimentation applications, demonstrating dynamic role assignment, information generation and exploitation, and emergence of learning barriers and coordination benefits.

\paragraph{Key Findings and Implications}

Our experimental analysis reveals that dynamic role assignment is a robust organizing principle of multi-agent learning systems. Across both scenarios, agents naturally differentiate into complementary roles enhancing collective information processing. Our results show that free-riding behavior does not lead to predicted inefficiencies—performance of some agents in larger networks substantially exceeds autarky levels. The computational approach illuminates fundamental tension between theoretical and practical approaches: while economic models envision agents developing ex ante strategies optimal across all states, reinforcement learning agents converge to ex post strategies optimized for specific realized states. This reveals limitations in directly applying computational methods to test theoretical predictions while suggesting opportunities for more realistic behavioral models.

\paragraph{Methodological Contributions and Limitations}

Our two-staged analysis methodology provides systematic approaches for extracting insights from multi-agent learning dynamics despite catastrophic forgetting. The discretization approach for continuous-time economic models treats Lévy processes through Euler-Maruyama schemes and maps continuous decisions to discrete action probabilities, providing computational implementation templates while preserving essential incentive structures. We constructed observed reward functions enabling reinforcement learning without direct reward signals and developed specialized transformer loss functions for Lévy process observations. Our implementation incorporates Graph Neural Networks with temporal and spatial attention mechanisms capturing network topology and temporal dependencies. We introduce theoretically grounded off-equilibrium asymmetric analysis methods moving beyond symmetric equilibria analysis, allowing heterogeneous agent behaviors and asymmetric strategy convergence. Despite these advances, catastrophic forgetting prevents direct measurement of predicted equilibrium strategies, requiring alternative analytical approaches, and computational complexity limits analysis to small networks and finite horizons, potentially missing asymptotic behaviors theoretical models emphasize.

\paragraph{Future Research Directions}

Several promising avenues emerge: developing continual learning techniques for social learning environments could address catastrophic forgetting through architectures maintaining separate policy components for different states; scaling to larger networks and longer horizons would enable asymptotic validation; extending to dynamic environments where underlying states change would broaden applicability; and investigating optimal information structures and network topologies could inform platform and institutional design.

\paragraph{Broader Impact and Conclusion}

This thesis demonstrates the value of interdisciplinary approaches bridging economic theory and computational methods. By preserving theoretical rigor while leveraging modern machine learning techniques, we gain insights into complex social phenomena neither approach could achieve alone. The insights have implications for financial markets, organizational learning, online social networks, and collaborative scientific endeavors. Dynamic role assignment suggests effective social learning systems naturally develop specialized roles enhancing collective information processing, providing guidance for platform design and institutional arrangements. Our POAMG framework and POLARIS algorithm contribute to multi-agent reinforcement learning by providing principled approaches for handling partial observability and strategic adaptation. The techniques may prove valuable for other applications requiring sophisticated multi-agent coordination under uncertainty. The convergence of economic theory and artificial intelligence presents exciting opportunities for advancing understanding of social learning and strategic behavior. This thesis provides theoretical foundations and computational tools for future researchers to illuminate rich dynamics of social learning in complex environments. While significant progress has been made, much work remains to fully realize this integration's potential, with the ultimate goal of better understanding how intelligent agents can learn from each other to make better decisions in an uncertain world.
